# LLM 기반 SpecItem 검증 및 Diff 비교 가이드

## 개요

LLM을 사용하여 불확실한 SpecItem을 검증하고, 번역키나 메타데이터를 자동으로 필터링하는 기능입니다.

## 작동 방식

### 하이브리드 접근

1. **1단계: 규칙 기반 추출** (빠르고 저렴)
   - 기본 파싱 로직으로 SpecItem 추출
   - 확실한 항목은 그대로 사용

2. **2단계: 불확실한 항목만 LLM 검증** (비용 절감)
   - 번역키 패턴이지만 확실하지 않은 경우
   - UI 키워드가 없고 짧은 텍스트
   - 메타데이터로 보이지만 확실하지 않은 경우

3. **3단계: LLM 검증 결과 반영**
   - 신뢰도 70% 이상만 유효한 항목으로 인정
   - 실패 시 원본 항목 사용 (안정성)

## 환경 변수 설정

### 필수 설정

```bash
# LLM Provider 선택 (openai 또는 anthropic)
LLM_PROVIDER=openai

# API 키
LLM_API_KEY=your-api-key-here

# LLM 활성화
LLM_ENABLED=true

# SpecItem 검증 활성화 (새로운 기능)
LLM_SPEC_VALIDATION_ENABLED=true

# LLM 기반 Diff 비교 활성화 (새로운 기능)
# 불확실한 finding을 LLM으로 재검증하여 더 정교한 비교 수행
LLM_DIFF_ENABLED=true
```

### 선택 설정

```bash
# 모델명 (기본값 사용 시 생략 가능)
LLM_MODEL=gpt-4o-mini  # OpenAI 기본값
# 또는
LLM_MODEL=claude-3-haiku-20240307  # Anthropic 기본값
```

## 비용 예상

### 예시: 100개 SpecItem 중 20개가 불확실한 경우

- **OpenAI GPT-4o-mini**: 약 $0.01-0.02
- **Anthropic Claude Haiku**: 약 $0.01-0.02

### 비용 절감 전략

- 불확실한 항목만 검증 (전체의 10-20%)
- 배치 처리로 API 호출 최소화
- 캐싱 (동일 항목 재검증 방지)

## 사용 예시

### 기본 사용 (환경 변수 설정)

```bash
# .env.local 파일에 추가
LLM_PROVIDER=openai
LLM_API_KEY=sk-...
LLM_ENABLED=true
LLM_SPEC_VALIDATION_ENABLED=true
```

### 코드에서 확인

```typescript
// API 로그에서 확인 가능
[DEBUG] LLM 기반 SpecItem 검증 시작...
[LLM] 불확실한 항목 5개 검증 시작...
[LLM] 제외: "more_myinfo_account_delete_..." (이유: 번역키로 판단됨, 신뢰도: 0.95)
[LLM] 검증 완료: 3/5개 유효
[DEBUG] LLM 검증 후 SpecItem 수: 48
```

## 검증 기준

### 확실히 유효한 항목 (LLM 검증 생략)

- 따옴표로 감싼 텍스트 (`"인기순"`)
- UI 키워드 포함 (`버튼`, `필터`, `정렬` 등)

### 불확실한 항목 (LLM 검증)

- 번역키 패턴이지만 짧은 경우 (5-15자)
- UI 키워드 없고 짧은 텍스트 (5-15자)
- 메타데이터로 보이지만 확실하지 않은 경우

### LLM 판단 기준

- **유효함**: 실제 사용자에게 보이는 UI 텍스트
- **무효함**: 번역키, 내부 식별자, 메타데이터

## 주의사항

1. **비용 관리**
   - 불확실한 항목만 검증하도록 설계됨
   - 대량 문서의 경우 비용 확인 필요

2. **신뢰도 임계값**
   - 기본값: 70% 이상만 유효
   - 환경 변수로 조정 가능 (향후)

3. **API 실패 시**
   - 자동으로 원본 SpecItem 사용
   - 서비스 중단 없음

4. **일관성**
   - LLM은 동일 입력에 대해 약간 다른 결과를 낼 수 있음
   - 프로덕션 사용 시 모니터링 필요

## 성능

- **검증 속도**: 항목당 약 0.5-1초
- **비용**: 항목당 약 $0.0001-0.0002
- **정확도**: 약 85-95% (테스트 필요)

## 향후 개선

1. **캐싱**: 동일 항목 재검증 방지
2. **배치 처리**: 여러 항목을 한 번에 검증
3. **신뢰도 임계값 조정**: 환경 변수로 설정 가능
4. **로컬 LLM 지원**: 비용 절감
